
爬虫：
	模拟浏览器向web服务器发送请求，获取服务器回发数据。分析，存储、使用。

	网络蜘蛛、网页机器人。

爬取流程：【重点】

	1. 明确目标。 URL

	2. 发送请求。获取服务器响应数据

	3. 提取有效信息。

	4. 存储、使用数据。

https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=0		1	下一页+50

https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=50		2

https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=100		3

https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=150		4

百度贴吧爬虫：

	1. 提示用户来指定 爬取的 起始、终止页。 （2-7）

	2. 分析网页，找寻分页器规律（下一页+50）。  对应 组织处每一页的 URL。

	3. 使用url向服务器请求一页数据。

		url := "xxxxx" +itoa(i-1)*50

	4. 提取服务器回发的一页数据。
		http.Get(url)

		resp.Body.read()

		result += string(buf[:n])    保存一个网页数据内容。

	5. 存储网页数据。

		封装函数 save2File(result, i)

			create、writeString((result)

	6. 循环 上述 2-5 步骤，抓取所有的网页数据。

	7. 关闭。

并发百度爬虫实现：

	1.  封装爬取一个页面的 指令 到 函数 spiderPage()

	2.  for i:= start; i<=end; i++ {
		go spiderPage()
	    }
		默认，主go程，先于子go程结束。

	3. 创建 channel 协调主、子go程执行先后顺序。  quit := make(chan int)

	4. 在 spiderPage结尾处，添加 channel 写事件。  每爬取完一个页面，写数据到channel通知，主go程。

	5. for i:= start; i<=end; i++ {
		<-quit
	    }		
		有多个子go程，对应多少次 写操作。有多少次读操作。

正则表达式： 跨语言的。

	在字符串数据中，匹配子串，筛选文本。

字符类：

	“.” 匹配任意一个字符 （ 默认不包含 \n ）	abc. 

	“[ ]” 匹配 [ ] 内任意一个字符 	[ab]d

	“-” 搭配 [ ] 使用， 指定范围	[0-9]m	[a-zA-Z0-9]

	“^” 搭配 [ ] 使用， 位于 [ ] 开头。表示匹配除 [ ] 以外的任意一个字符

数量限定符：

	“?”  匹配它前面的单元 0-1 次		[0-9]?\.[0-9]

	“+” 匹配它前面的单元 1-N 次		[0-9]+\.[0-9]

	“*”  匹配它前面的单元 0-N 次		[0-9]*+\.[0-9]

	“{N}” 匹配它前面的单元  N 次		[0-9]{2}\.[0-9]{3}

	“{N,}” 匹配它前面的单元  N 次		[0-9]{2}\.[0-9]{3}	

	“{N,M}”匹配它前面的单元  N - M 次	[0-9]{2,5}\.[0-9]{1,3}

其他特殊字符：

	“\”: 转义字符。	作用1： 将字符与“\”组合，形成新意。 	‘\n’'\d'

			作用2： 将字符与“\”组合，还原字符本身意。 \.  ---> "."

	“（）”单元设定符。  被（）包裹的正则表达式，被看做一个单元，可以对该单元使用 数量限定符。

	“|” 连接符。连接两个表达式，表或关系。 	如： h(is|im)

Go语言使用正则表达式：【重点】

	1. 编译解析正则：

		func MustCompile(str string) *Regexp

		参：使用反引号“ ` ”包裹的 正则表达式

		返回：编译后的 正则表达式。 （结构体格式 ―― 能被go编译器识别。）

	2. 提取需要数据信息

		func (re *Regexp) FindAllStringSubmatch(s string, n int) [][]string

		参1： 待搜索的原数据。

		参2： 指定匹配次数。-1 ： 全部。

		返回值： 匹配成功的 数据对象。

			返回 [][]string

			每row数据：从正文中， 正则表达式匹配出的结果。 

			每row上有 2 个 元素 string1， string2：

				string1： 带有匹配参考项的 数据。 【0】

				string2： 不带有匹配参考项的 数据。  【1】 使用

待参考项的多行字符串匹配：(?s:(.*?))

	1. 单行模式：（?s）: 	"?" 单行模式提示符。

			“s”促使，“.”匹配“\n”

	2. (.*): 匹配 前面的单元 >=0次， 越【多】越好。 匹配：从 1-7

		<div>xxx</div><div>YYY</div><div>ZZZ</div>
		   1            2        3       4      5        6              7

	    (.*?): 匹配 前面的单元 >=0次， 越【少】越好。 匹配：从 1-2		

	【结论】： 在具备起始匹配参考项、终止匹配参考项的数据源中，提取多行匹配数据。 方法： `起始匹配参考项(?s:(.*?))终止匹配参考项`

		 e.g.  <div>(?s:(.*))</div>

-----------------------------------------

爬取豆瓣电影：

	1. 明确url： 分页器规律：

		https://movie.douban.com/top250?start=0&filter=		1

		https://movie.douban.com/top250?start=25&filter=		2		下一页+25

		https://movie.douban.com/top250?start=50&filter=		3

		https://movie.douban.com/top250?start=75&filter=		4

	横向爬取：找寻分页器规律。按页爬取数据。

	纵向爬取：提取一整页数据。 按条目爬取。

		电影名：`<img width="100" alt="（电影名称）"`

		评分：`<span class="rating_num" property="v:average">(评分)</span>`

		人数：`<span>（人数）人评价</span>`

	2. 发送请求  
	3. 筛选数据  
	4. 存储、使用数据。

分析实现思路：

	1.  提示用户 输入起始、终止页面  start、end

	2.  创建函数 working ，根据起始、终止页。组织每一页的url
		
		for i:=start; i<=end; i++ {
			url := "https://movie.douban.com/top250?start=0&filter="  + itoa((i-1)*25)
		}

	3. 封装 函数 spiderPage（） 爬取一个页面 （25条电影描述信息。）

	4. 实现 spiderPage ：

		1） 根据 url 获取网页信息  

			resp := http.Get()

		2)   for 循环 resp.Body.read(buf)

		3)   result += string(buf[:n])   ―― 保存 所有的 25 部电影描述信息。

		4)   编译、解析正则表达式。提取 “电影名”

			 MustCompile(`<img width="100" alt="(?s:.*?)"`) 

			alls :=FindAllStringSubmatch（result）
				取下标为【1】

		5)   编译、解析正则表达式。提取 “分数”

			 MustCompile(`。。。`) 

			alls := FindAllStringSubmatch（result）
				取下标为【1】

		6)   编译、解析正则表达式。提取 “评价人数”

			 MustCompile(` 。。。`) 

			FindAllStringSubmatch（result）
				取下标为【1】

	5. 封装 save2File（）函数，将爬取到的每一个网页数据中的 25 条 电影的 “电影名”、“分数”、“评价人数” 写出成一个文件。

	6.  实现 并发。  go spiderPage（）

	7.  创建 channel 协调主、子go程执行先后顺序。

		























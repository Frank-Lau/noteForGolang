
爬虫：
	模拟浏览器向web服务器发送请求，获取服务器回发数据。分析，存储、使用。

	网络蜘蛛、网页机器人。

爬取流程：【重点】

	1. 明确目标。 URL 

	2. 发送请求。获取服务器响应数据

	3. 提取有效信息。

	4. 存储、使用数据。

https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=0		1	下一页+50

https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=50		2

https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=100		3

https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=150		4

百度贴吧爬虫：

	1. 提示用户来指定 爬取的 起始、终止页。 （2-7）

	2. 分析网页，找寻分页器规律（下一页+50）。  对应 组织处每一页的 URL。

	3. 使用url向服务器请求一页数据。

		url := "xxxxx" +itoa(i-1)*50

	4. 提取服务器回发的一页数据。
		http.Get(url)

		resp.Body.read()

		result += string(buf[:n])    保存一个网页数据内容。

	5. 存储网页数据。

		封装函数 save2File(result, i)

			create、writeString((result)

	6. 循环 上述 2-5 步骤，抓取所有的网页数据。

	7. 关闭。

并发百度爬虫实现：

	1.  封装爬取一个页面的 指令 到 函数 spiderPage()

	2.  for i:= start; i<=end; i++ {
		go spiderPage()
	    }
		默认，主go程，先于子go程结束。

	3. 创建 channel 协调主、子go程执行先后顺序。  quit := make(chan int)

	4. 在 spiderPage结尾处，添加 channel 写事件。  每爬取完一个页面，写数据到channel通知，主go程。

	5. for i:= start; i<=end; i++ {
		<-quit
	    }		
		有多个子go程，对应多少次 写操作。有多少次读操作。

正则表达式： 跨语言的。

	在字符串数据中，匹配子串，筛选文本。

字符类：
	“.” 匹配任意一个字符 （ 默认不包含 \n ）	abc. 

	“[ ]” 匹配 [ ] 内任意一个字符 	[ab]d

	“-” 搭配 [ ] 使用， 指定范围	[0-9]m	[a-zA-Z0-9]

	“^” 搭配 [ ] 使用， 位于 [ ] 开头。表示匹配除 [ ] 以外的任意一个字符

数量限定符：

	“?”  匹配它前面的单元 0-1 次		[0-9]?\.[0-9]

	“+” 匹配它前面的单元 1-N 次		[0-9]+\.[0-9]

	“*”  匹配它前面的单元 0-N 次		[0-9]*+\.[0-9]

	“{N}” 匹配它前面的单元  N 次		[0-9]{2}\.[0-9]{3}

	“{N,}” 匹配它前面的单元至少  N 次	[0-9]{2}\.[0-9]{3}	

	“{N,M}”匹配它前面的单元  N - M 次	[0-9]{2,5}\.[0-9]{1,3}

其他特殊字符：

	“\”: 转义字符。	作用1： 将字符与“\”组合，形成新意。 	‘\n’‘\r’'\d'   '\t'

			作用2： 将字符与“\”组合，还原字符本身意。 \.  ---> "."   \"   ----> " " "

	“（）”单元设定符。  被（）包裹的正则表达式，被看做一个单元，可以对该单元使用 数量限定符。

	“|” 连接符。连接两个表达式，表或关系。 	如： h(is|im|er)

Go语言使用正则表达式：【重点】

	1. 编译解析正则：

		func MustCompile(str string) *Regexp

		参：使用反引号“ ` ”包裹的 正则表达式

		返回：编译后的 正则表达式。 （结构体格式 ―― 能被go编译器识别。）

	2. 提取需要数据信息

		func (re *Regexp) FindAllStringSubmatch(s string, n int) [][]string

		参1： 待搜索的原数据。

		参2： 指定匹配次数。-1 ： 全部。

		返回值： 匹配成功的 数据对象。

			返回 [][]string

			每row数据：从正文中， 正则表达式匹配出的结果。 

			每row上有 2 个 元素 string1， string2：

				string1： 带有匹配参考项的 数据。 【0】

				string2： 不带有匹配参考项的 数据。  【1】 使用

待参考项的多行字符串匹配：(?s:(.*?)) 

	1. 单行模式：（?s）: 	"?" 单行模式提示符。

			“s”促使，“.”匹配“\n”

	2. (.*): 匹配 前面的单元 >=0次， 越【多】越好。 匹配：从 1-7

		<div>xxx</div><div>YYY</div><div>ZZZ</div>
		   1            2        3       4      5        6              7

	    (.*?): 匹配 前面的单元 >=0次， 越【少】越好。 匹配：从 1-2		

	【结论】： 在具备起始匹配参考项、终止匹配参考项的数据源中，提取多行匹配数据。 方法： `起始匹配参考项(?s:(.*?))终止匹配参考项`

		 e.g.  <div>()</div>

-----------------------------------------

爬取豆瓣电影：

	1. 明确url： 分页器规律：

		https://movie.douban.com/top250?start=0&filter=		1

		https://movie.douban.com/top250?start=25&filter=		2		下一页+25

		https://movie.douban.com/top250?start=50&filter=		3

		https://movie.douban.com/top250?start=75&filter=		4

	横向爬取：找寻分页器规律。按页爬取数据。

	纵向爬取：提取一整页数据。 按条目爬取。

		电影名：`<img width="100" alt="（电影名称）"`

		评分：`<span class="rating_num" property="v:average">(评分)</span>`

		人数：`<span>（人数）人评价</span>`

	2. 发送请求  
	3. 筛选数据  
	4. 存储、使用数据。

分析实现思路：

	1.  提示用户 输入起始、终止页面  start、end

	2.  创建函数 working ，根据起始、终止页。组织每一页的url
		
		for i:=start; i<=end; i++ {
			url := "https://movie.douban.com/top250?start=0&filter="  + itoa((i-1)*25)
		}

	3. 封装 函数 spiderPage（） 爬取一个页面 （ 25条电影描述信息。）

	4. 实现 spiderPage ：

		1） 根据 url 获取网页信息  

			resp := http.Get()

		2)   for 循环 resp.Body.read(buf)

		3)   result += string(buf[:n])   ―― 保存 所有的 25 部电影描述信息。

		4)   编译、解析正则表达式。提取 “电影名”

			 MustCompile(`<img width="100" alt="(?s:.*?)"`) 

			alls :=FindAllStringSubmatch（result）
				取下标为【1】

		5)   编译、解析正则表达式。提取 “分数”

			 MustCompile(`。。。`) 

			alls := FindAllStringSubmatch（result）
				取下标为【1】

		6)   编译、解析正则表达式。提取 “评价人数”

			 MustCompile(` 。。。`) 

			FindAllStringSubmatch（result）
				取下标为【1】

	5. 封装 save2File（）函数，将爬取到的每一个网页数据中的 25 条 电影的 “电影名”、“分数”、“评价人数” 写出成一个文件。

	6.  实现 并发。  go spiderPage（）

	7.  创建 channel 协调主、子go程执行先后顺序。

---------------------------------------

爬取段子： 捧腹网。

	1. 获取 url：	

		https://www.pengfu.com/xiaohua_1.html

		https://www.pengfu.com/xiaohua_2.html

		https://www.pengfu.com/xiaohua_3.html  	下一页+1

	2.  每一个段子对应的 新页面的 url ：

		提取特性： `<h1 class="dp-b"><a href="(url连接)"`

	3.  找寻 标题 检索特性：
		
		`<h1>(title)</h1>`   抓取第一个出现的 <h1>

	4. 找寻 正文 检索特性：

		`<div class="content-txt pt10">(content)<a id="prev"`

实现流程：
	1. 提示用户 输入起始、终止页面  start、end

	2. 创建函数 working ，根据起始、终止页。组织每一页的url

	3. 封装 spiderJokeS 函数，循环调用，爬取每一个页面（ 含有10个段子 ）

	4. 实现 spiderJokeS 

		1） 组织页面URL

		2）封装 httpGetJoke，传入 url 抓取笑话页面的所有内容。  http.Get/ resp.Body.read/ result+=string(buf[:n]) / return 

		3)  编译解析正则， 提取每一个段子的 url	

		4）封装 spiderOneJokePage(url) 函数，循环调用，爬取每一个段子页面（含有1个段子）

	5. 实现spiderOneJokePage

		1）使用 httpGetJoke，传入 url 抓取一个笑话页面的所有内容。

		2）编译解析正则， 提取段子的 title

		3）编译解析正则， 提取段子的 content     

		4）替换掉 title 和 content中的无用符号。 replace  。将 title 和 content返回传出（一个titile/一个connent）

	6. 在 spiderJokeS函数中 创建 切片 titleS 和 contentS。 存储页面的 10 个 标题和对应内容。

	7. 将spiderOneJokePage调用传出的每一个 title、content 追加到且末尾。

	8. 封装并实现 saveToFile（ titleS ， contentS）保存 10 个笑话及内容。

	9. 创建并发 go ， 创建channel 协调执行先后顺序。

斗鱼颜值图片爬取：

	每张图片的 url 抓取正则 ： `data-original="(?s:(.*?))"`


【总结】：爬虫是一个“领域”。想做爬虫开发，挣钱！除了我们介绍的这部分知识以外。需要：

		分布式服务器：Nginx、fastDFS ... 

		分布式存储

		数据库：MySQL、MongoDB、Redis ... 

		JSON格式数据存储

		Teleport：堡垒机 	等技术手段辅助完成。

	大约 一个 13-18 天的项目。―― python、php、Perl、JavaScript 的饭碗。

1. 掌握 go 爬虫使用方法后，再来学习此框架：

	Golang 网络爬虫框架gocolly 。 快速提高 go 爬取网页的效率。

2. https://www.oschina.net/p/pholcus  ―― Go 编写的重量级爬虫软件（开源中国）。 


反爬取:(谷歌浏览器)

1.不同于之前的爬取点击查看网页源代码,此处需要点击	检查	->	2.把下拉条拉倒最下方	->	3点击NetWork(这是随着随着鼠标的移动,

	控制台会出现大量数据)	->	点击灰色图标clear将鼠标移动产生的数据清空	->再次点击屏幕下方对应页码将会出现对应页码的数字,双击

	将拿到真正的URL,找到分页器规律























